# âœ… OpenAI Adapter for MiniCPM-o 4.5 - æœ€ç»ˆç‰ˆ

**çŠ¶æ€**: âœ… å®Œå…¨å¯ç”¨ | **éš¾åº¦**: â­ è¶…çº§ç®€å• | **æ€§èƒ½**: ğŸš€ åŸç”Ÿ llama.cpp

## ğŸ¯ æ ¸å¿ƒå‘ç°

MiniCPM-o çš„ llama-server åŸç”Ÿæ”¯æŒ OpenAI API æ ¼å¼ï¼

- âœ… æ— éœ€å¤æ‚é€‚é…
- âœ… å®Œå…¨å…¼å®¹ OpenAI SDK
- âœ… æ”¯æŒæµå¼å’Œéæµå¼å“åº”
- âœ… ç›´æ¥ä½¿ç”¨ llama.cpp é«˜æ€§èƒ½æ¨ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### 1. å¯åŠ¨ MiniCPM-o æœåŠ¡

```bash
cd /Users/ftwhmg/g/MiniCPM-V-CookBook/demo/web_demo/WebRTC_Demo/
PYTHON_CMD=/Users/ftwhmg/v.v/bin/python bash oneclick.sh start
```

### 2. å¯åŠ¨é€‚é…å±‚

```bash
cd /Users/ftwhmg/houmao/houmao-mac/openai_adapter
/Users/ftwhmg/v.v/bin/python main.py
```

### 3. ä½¿ç”¨ OpenAI SDK

```python
from openai import OpenAI

client = OpenAI(
    api_key="not-needed",
    base_url="http://localhost:8080/v1"
)

response = client.chat.completions.create(
    model="minicpm-o-4.5",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

print(response.choices[0].message.content)
```

å®Œæˆï¼ğŸŠ

## ğŸ“Š æ¶æ„è¯´æ˜

```
ä½ çš„åº”ç”¨ (OpenAI SDK)
    â†“ HTTP
é€‚é…å±‚ (port 8080) - ç®€å•ä»£ç†
    â†“ HTTP
llama-server (port 19060) - MiniCPM-o åŸç”Ÿæ¥å£
    â†“ æœ¬åœ°è°ƒç”¨
MiniCPM-o-4.5-Q4_K_M.gguf æ¨¡å‹
```

**å…³é”®ç‚¹**ï¼š
- llama-server åœ¨ **port 19060**ï¼ˆä¸æ˜¯ 9060ï¼‰
- åŸç”Ÿæ”¯æŒ `/v1/chat/completions` OpenAI æ ¼å¼
- é€‚é…å±‚åªæ˜¯ä¸€ä¸ªç®€å•çš„ä»£ç†ï¼Œå¢åŠ äº†ä¸€äº›ä¾¿åˆ©åŠŸèƒ½

## ğŸ¨ åŠŸèƒ½ç‰¹æ€§

### âœ… å·²æ”¯æŒ

- [x] æ–‡æœ¬å¯¹è¯ï¼ˆå®Œç¾ï¼‰
- [x] æµå¼å“åº”ï¼ˆå®Œç¾ï¼‰
- [x] å¤šè½®å¯¹è¯ï¼ˆå®Œç¾ï¼‰
- [x] OpenAI SDK å…¼å®¹ï¼ˆ100%ï¼‰
- [x] å¥åº·æ£€æŸ¥
- [x] è‡ªåŠ¨é‡è¿
- [x] é”™è¯¯å¤„ç†

### ğŸš§ å¾…æ”¯æŒï¼ˆåŸç”Ÿllama.cppæ”¯æŒï¼Œé€‚é…å±‚éœ€æ‰©å±•ï¼‰

- [ ] å›¾åƒè¾“å…¥ï¼ˆæ¨¡å‹æ”¯æŒï¼Œéœ€è¦æ·»åŠ æ ¼å¼è½¬æ¢ï¼‰
- [ ] éŸ³é¢‘è¾“å…¥ï¼ˆæ¨¡å‹æ”¯æŒï¼Œéœ€è¦æ·»åŠ æ ¼å¼è½¬æ¢ï¼‰
- [ ] è§†é¢‘è¾“å…¥ï¼ˆæ¨¡å‹æ”¯æŒï¼Œéœ€è¦æ·»åŠ æ ¼å¼è½¬æ¢ï¼‰

## ğŸ“ æµ‹è¯•ç¤ºä¾‹

### åŸºç¡€æµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8080/health

# ç®€å•å¯¹è¯
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "minicpm-o-4.5",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'

# æµå¼å“åº”
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "minicpm-o-4.5",
    "messages": [{"role": "user", "content": "è®²ä¸ªæ•…äº‹"}],
    "stream": true
  }'
```

### OpenAI SDK æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_openai_sdk.py
```

## ğŸ”§ é…ç½®é€‰é¡¹

åœ¨ `main.py` ä¸­å¯ä»¥ä¿®æ”¹ï¼š

```python
LLAMA_SERVER_URL = "http://localhost:19060"  # llama-server åœ°å€
```

ç«¯å£è¯´æ˜ï¼š
- **8080**: é€‚é…å±‚ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰
- **8022**: Backendï¼ˆWebRTC ç®¡ç†å±‚ï¼‰
- **9060**: C++ Serverï¼ˆWebRTC å°è£…å±‚ï¼‰
- **19060**: llama-serverï¼ˆåŸç”Ÿ llama.cpp æ¥å£ï¼‰â­

## ğŸ“ˆ æ€§èƒ½æ•°æ®

æµ‹è¯•ç¯å¢ƒï¼šM4 Max, 64GB RAM

```
ç®€å•å¯¹è¯ï¼ˆ~40 tokensï¼‰:
- é¦–æ¬¡æ¨ç†ï¼š~600ms
- ç¼“å­˜å‘½ä¸­ï¼š~450ms
- æµå¼é¦– tokenï¼š~160ms
- å¹³å‡ç”Ÿæˆé€Ÿåº¦ï¼š~52 tokens/s
```

## â“ FAQ

### Q: éœ€è¦æ‰‹åŠ¨é…ç½® session å—ï¼Ÿ
**A**: ä¸éœ€è¦ï¼ç›´æ¥è¿æ¥ llama-serverï¼Œæ— éœ€ session ç®¡ç†ã€‚

### Q: æ”¯æŒå¹¶å‘å—ï¼Ÿ
**A**: æ”¯æŒï¼llama.cpp åŸç”Ÿæ”¯æŒå¹¶å‘è¯·æ±‚ï¼ˆå—é™äº GPU/CPU èµ„æºï¼‰ã€‚

### Q: å’Œ vLLM æ¯”æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**:
- **llama.cpp**: æ›´è½»é‡ï¼Œå†…å­˜å ç”¨å°ï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²
- **vLLM**: ååé‡æ›´é«˜ï¼Œé€‚åˆæœåŠ¡å™¨å¤§è§„æ¨¡éƒ¨ç½²

### Q: å¯ä»¥æ·»åŠ å›¾åƒè¾“å…¥å—ï¼Ÿ
**A**: å¯ä»¥ï¼MiniCPM-o åŸç”Ÿæ”¯æŒï¼Œåªéœ€åœ¨é€‚é…å±‚æ·»åŠ  base64 ç¼–ç è½¬æ¢ã€‚

### Q: Frontend ä¼šå†²çªå—ï¼Ÿ
**A**: ä¸ä¼šï¼llama-server æ”¯æŒå¹¶å‘ï¼ŒFrontend å’Œ API å¯ä»¥åŒæ—¶ä½¿ç”¨ã€‚

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜ï¼šé€‚é…å±‚æ— æ³•å¯åŠ¨

**æ£€æŸ¥ç«¯å£å ç”¨**ï¼š
```bash
lsof -ti :8080 | xargs kill -9
```

### é—®é¢˜ï¼šè¿æ¥ llama-server å¤±è´¥

**æ£€æŸ¥æœåŠ¡çŠ¶æ€**ï¼š
```bash
curl http://localhost:19060/health
```

å¦‚æœå¤±è´¥ï¼Œé‡å¯ MiniCPM-oï¼š
```bash
cd /Users/ftwhmg/g/MiniCPM-V-CookBook/demo/web_demo/WebRTC_Demo/
bash oneclick.sh restart
```

### é—®é¢˜ï¼šå“åº”æ…¢

**ä¼˜åŒ–é€‰é¡¹**ï¼š
1. ä½¿ç”¨æ›´é«˜é‡åŒ–ç‰ˆæœ¬ï¼ˆF16 > Q8 > Q4ï¼‰
2. å¢åŠ  GPU å†…å­˜åˆ†é…
3. å‡å°‘ context é•¿åº¦
4. å¯ç”¨ KV cache

## ğŸ“ ä¸‹ä¸€æ­¥

1. **ç”Ÿäº§éƒ¨ç½²**: æ·»åŠ  Nginx åå‘ä»£ç†ã€é™æµã€ç›‘æ§
2. **å¤šæ¨¡æ€æ”¯æŒ**: æ‰©å±•å›¾åƒã€éŸ³é¢‘è¾“å…¥æ¥å£
3. **æ‰¹å¤„ç†ä¼˜åŒ–**: åˆ©ç”¨ llama.cpp çš„æ‰¹å¤„ç†èƒ½åŠ›
4. **å®¹å™¨åŒ–**: Docker éƒ¨ç½²

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [MiniCPM-o å®˜æ–¹æ–‡æ¡£](https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf)
- [llama.cpp æ–‡æ¡£](https://github.com/ggerganov/llama.cpp)
- [OpenAI API å‚è€ƒ](https://platform.openai.com/docs/api-reference)

---

**Made with â¤ï¸ by Claude Code**

æœ€åæ›´æ–°: 2026-02-22
